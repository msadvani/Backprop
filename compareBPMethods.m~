%Run tests on the different backpropagation algorithms over different
%classes of networks, etc. for a detailed comparison.

close all;
clear all;

%% Run a basic comparison error versus iteration

% disp('running normal backprop');
% [errNonLoc]=backprop(randn(5,1),3,.1,5000);
% %plot([1:length(errNonLoc)],errNonLoc)
% 
% %hold on;
% disp('running the local noisy backprop');
% %Note this is based on an error
% [errLoc]=localNoisyBPSim(randn(5,1),3,sqrt(.1),1,10,5000);
% %For a fair comparison step size = epislon^2 *gradstep_noisy
% plot([1:length(errNonLoc)],errNonLoc,'bo',[1:length(errLoc)],errLoc,'r+')


%% Compare error at the end of some fixed iteration length for different noise levels


% baseStep = .01;
% [errNonLoc]=backprop(randn(5,1),10,baseStep,3000);
% %plot([1:length(errNonLoc)],errNonLoc)
% 
% hold on;
% %[errLoc]=localNoisyBPSim(randn(5,1),10,sqrt(baseStep),1,10,3000);
% [errLoc]=localNoisyBPSimAnneal(randn(5,1),10,sqrt(baseStep),.4,1,40,3000);
% %For a fair comparison step size = epislon^2 *gradstep_noisy
%  plot([1:length(errNonLoc)],errNonLoc,'bo',[1:length(errLoc)],errLoc,'r+')
% 
% xlabel('Number of Gradient Steps')
% ylabel('Error')
% title('Comparison of Backprop Algorithms')
% 
% legend('normal backprop','local noisy backprop');


%% Just for testing things work


%backpropOnline(rand(10,5),10,.01,10000); 


%Local version seems more robust to depth of network. Not really sure why,
%Maybe because it doesn't change a layer until it is noticed a correlation,
%so fewer false positives (May work signicantly better in deeper networks


%localNoisyBPSimOnline(rand(10,5),3,.1,1,50,10000);

%localNoisyBPSimOnline(rand(10,5),10,.1,1,50,10000);
backpropOnline(rand(10,5),40,.01,10000); 
localNoisyBPSimOnline(rand(10,5),10,.1,1,50,10000);



%Try writing an online version of noisy backprop, probably the noise helps
%avoid local minima, and may be better than vanilla backprop. I don't know
%how this might change if I added a momentum term...


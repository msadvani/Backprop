%Single layer backpropagation (non-noisy)


M=100; %Number neurons per layer

s = randn(M,1); %Set input


W_ideal = randn(M); %Teacher network
yTarg = W_ideal*s; %Target (from teacher net)




W0 = randn(M);
W = W0; %Initialize network randomly

numCnt = 10000; %Number of steps to take in gradient descent
numAvg=1;   %Number of averages to computed the appropriate gradient direction
step =.001; %Gradient step size (could also be updated during proccess) 


%Larger noise might imply (smaller step size [more time averaging] to
%converge...Particularly when there are non-linearities, the stepsizes will
%need to be adjusted more.

%epsilon = .447; %Standard dev of noise
epsilon = .01/sqrt(M);


for cnt = 1:numCnt  
    dWset = zeros(M,M,numAvg);
    [cnt,numCnt]
    %Maybe the distinction is not too important if you don't even have
    %non-linearities in the system.
    
    for avgcnt =1:numAvg
        eta1 = epsilon*randn(M,1);
        eta2 = epsilon*randn(M,1);
        x = s+eta1;
        y = W*x + eta2;
        

        dWset(:,:,avgcnt) = (yTarg - y)*x';    
    end
    dW = mean(dWset,3);
    W = W + step*dW;  
end


yTarg-W*s
%PRINT ESTIMATION ERROR 
norm(yTarg-W*s)



